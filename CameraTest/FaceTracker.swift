//
//  FaceTracker.swift
//  CameraTest
//
//  Created by å°é¦¬å¤§ on 2020/08/30.
//  Copyright Â© 2020 å°é¦¬å¤§. All rights reserved.
//

import Foundation
import UIKit
import AVFoundation

public class FaceTracker: NSObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    let captureSession = AVCaptureSession()
    //  ä½¿ç”¨ã™ã‚‹ã‚«ãƒ¡ãƒ©ã‚’ãƒ•ãƒ­ãƒ³ãƒˆã‚«ãƒ¡ãƒ©ã«è¨­å®š
    let videoDevice = AVCaptureDevice.default(_: .builtInWideAngleCamera, for: AVMediaType.video, position: .front)
    var videoOutput = AVCaptureVideoDataOutput()
    var view: UIView
    private var findface: (_ arr: Array<CGRect>) -> Void
    let imageState: UILabel
    /** é¡”æ¤œçŸ¥å›æ•°*/
    var detectFaceCount: Int = 0
    /** èªè¨¼å¯èƒ½ãªé¡”ã®è§’åº¦ã®ç¯„å›²ã€€*/
    let validFaceAngleRange: ClosedRange<Float> = -2.5...2.5
    /** èªè¨¼å®Œäº†ã«å¿…è¦ãªæˆåŠŸå›æ•°*/
    let detectEndCount: Int = 10
    
    required init (viewForDisplay: UIView, findface: @escaping (_ arr: Array<CGRect>) -> Void, imageState: UILabel) {
        self.view = viewForDisplay
        self.findface = findface
        self.imageState = imageState
        super.init()
        self.initialize()
    }
    
    /** åˆæœŸå‡¦ç† */
    private func initialize() {
        // ã‚«ãƒ¡ãƒ©æ˜ åƒã®ç™»éŒ²
        do {
            let videoInput = try AVCaptureDeviceInput(device: self.videoDevice!) as AVCaptureDeviceInput
            self.captureSession.addInput(videoInput)
        } catch let error as NSError {
            print(error)
        }
        self.videoOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as AnyHashable as! String: Int(kCVPixelFormatType_32BGRA)]
        
        // æ¯ãƒ•ãƒ¬ãƒ¼ãƒ å®Ÿè¡Œã™ã‚‹ãƒ‡ãƒªã‚²ãƒ¼ãƒˆç™»éŒ²
        let queue: DispatchQueue = DispatchQueue(label: "myqueue", attributes: .concurrent)
        self.videoOutput.setSampleBufferDelegate(self, queue: queue)
        self.videoOutput.alwaysDiscardsLateVideoFrames = true
        self.captureSession.addOutput(self.videoOutput)
        
        // ã‚«ãƒ¡ãƒ©æ˜ åƒã‚’æå†™ã™ã‚‹Viewã‚’æŒ‡å®š
        let videoLayer: AVCaptureVideoPreviewLayer = AVCaptureVideoPreviewLayer(session: self.captureSession)
        videoLayer.frame = self.view.bounds
        videoLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill
        self.view.layer.addSublayer(videoLayer)
        
        // ã‚«ãƒ¡ãƒ©å‘ãã‚’å®šç¾©
        for connection in self.videoOutput.connections {
            let conn = connection
            if conn.isVideoOrientationSupported {
                conn.videoOrientation = AVCaptureVideoOrientation.portrait
            }
        }
        
        self.captureSession.startRunning()
        
        AudioServicesPlaySystemSound(1113)
    }
    
    /** Bufferã‚’UIImageã¸å¤‰æ› */
    private func imageFromSampleBuffer(sampleBuffer: CMSampleBuffer) -> UIImage {
        let imageBuffer: CVImageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)!
        CVPixelBufferLockBaseAddress(imageBuffer, CVPixelBufferLockFlags(rawValue: 0))
        let baseAddress = CVPixelBufferGetBaseAddressOfPlane(imageBuffer, 0)
        let bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer)
        let width = CVPixelBufferGetWidth(imageBuffer)
        let height = CVPixelBufferGetHeight(imageBuffer)
        let colorSpace = CGColorSpaceCreateDeviceRGB()
        let bitmapInfo = (CGBitmapInfo.byteOrder32Little.rawValue | CGImageAlphaInfo.premultipliedFirst.rawValue)
        
        let context = CGContext(
            data: baseAddress,
            width: width,
            height: height,
            bitsPerComponent: 8,
            bytesPerRow: bytesPerRow,
            space: colorSpace,
            bitmapInfo: bitmapInfo
        )
        let imageRef = context!.makeImage()
        CVPixelBufferUnlockBaseAddress(imageBuffer, CVPixelBufferLockFlags(rawValue: 0))
        let resultImage: UIImage = UIImage(cgImage: imageRef!)
        return resultImage
    }
    
    /** ã‚«ãƒ¡ãƒ©æ˜ åƒã‚’å–å¾— */
    public func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        DispatchQueue.main.sync(execute: {
            // æ˜ åƒã‚’Bufferã‹ã‚‰CIImageã¸å¤‰æ›
            let image: UIImage = self.imageFromSampleBuffer(sampleBuffer: sampleBuffer)
            let ciimage: CIImage! = CIImage(image: image)
            
            // é¡”ã‚’æ¤œçŸ¥ã™ã‚‹ã€‚å‡¦ç†é€Ÿåº¦ã‚’é‡è¦–ã—ã€ç²¾åº¦ã¯ä½ãè¨­å®šã€‚
            let detector: CIDetector = CIDetector(
                ofType: CIDetectorTypeFace,
                context: nil,
                options: [
                    CIDetectorAccuracy: CIDetectorAccuracyHigh
                ])!
            let faces = detector.features(in: ciimage, options: [CIDetectorEyeBlink: true])
            
            // facesã«é¡”ã¨ã—ã¦èªè­˜ã—ãŸç”»åƒãŒæ ¼ç´ã•ã‚Œã‚‹
            if faces.count == 0 {
                ResetDetectCount("æ­£é¢ã‚’å‘ã„ã¦é¡”å…¨ä½“ã‚’ã‚«ãƒ¡ãƒ©ã«å†™ã—ã¦ãã ã•ã„")
                return
            } else if faces.count == 1 {
                //var _ : CIFaceFeature = CIFaceFeature()
                let face = faces[0] as! CIFaceFeature
                if (!face.hasMouthPosition) {
                    ResetDetectCount("ã‚«ãƒ¡ãƒ©ã«ã¯é¡”å…¨ä½“ã‚’å†™ã—ã¦ãã ã•ã„(å£ãŒå†™ã£ã¦ã„ãªã„)")
                    return
                }
                if (!face.hasFaceAngle) {
                    ResetDetectCount("ã‚«ãƒ¡ãƒ©ã«ã¯æ­£é¢ã‚’å‘ã„ãŸé¡”ã‚’å†™ã—ã¦ãã ã•ã„(è§’åº¦ãŒãŠã‹ã—ã„)")
                    return
                }
                if (!face.hasLeftEyePosition) {
                    ResetDetectCount("ã‚«ãƒ¡ãƒ©ã«ã¯é¡”å…¨ä½“ã‚’å†™ã—ã¦ãã ã•ã„(å·¦ç›®ãŒå†™ã£ã¦ã„ãªã„)")
                    return
                }
                if (!face.hasRightEyePosition) {
                    ResetDetectCount("ã‚«ãƒ¡ãƒ©ã«ã¯é¡”å…¨ä½“ã‚’å†™ã—ã¦ãã ã•ã„(å³ç›®ãŒå†™ã£ã¦ã„ãªã„)")
                    return
                }
                if (face.leftEyeClosed) {
                    ResetDetectCount("ä¸¡ç›®ã‚’é–‹ã„ã¦ãã ã•ã„(å·¦ç›®ãŒé–‰ã˜ã¦ã„ã‚‹)")
                    return
                }
                if (face.rightEyeClosed) {
                    ResetDetectCount("ä¸¡ç›®ã‚’é–‹ã„ã¦ãã ã•ã„(å³ç›®ãŒé–‰ã˜ã¦ã„ã‚‹)")
                    return
                }
                if (!validFaceAngleRange.contains(face.faceAngle)) {
                    ResetDetectCount("ã‚«ãƒ¡ãƒ©ã«ã¯æ­£é¢ã‚’å‘ã„ãŸé¡”ã‚’å†™ã—ã¦ãã ã•ã„(è§’åº¦ãŒãŠã‹ã—ã„)")
                    return
                }
                
                if (detectFaceCount == detectEndCount) {
                    AudioServicesPlaySystemSound(1002)
                }
                if (detectFaceCount >= detectEndCount) {
                    DetectOk("é¡”èªè­˜OKğŸ˜\n(æˆåŠŸå›æ•°:\(detectFaceCount)/è§’åº¦:\(face.faceAngle))")
                } else {
                    DetectOk("é¡”ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚ãã®ã¾ã¾ãŠå¾…ã¡ãã ã•ã„ã€‚\n(æˆåŠŸå›æ•°:\(detectFaceCount)/è§’åº¦:\(face.faceAngle))")
                }
            } else if faces.count >= 2 {
                ResetDetectCount("ã‚«ãƒ¡ãƒ©ã«ã¯ï¼‘äººã®é¡”ã ã‘ãŒå†™ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„")
                return
            }
            return
        })
    }
    
    private func DetectOk(_ message: String) {
        detectFaceCount += 1
        self.imageState.textColor = UIColor.systemGreen
        self.imageState.text = message
    }
    
    private func ResetDetectCount(_ message: String) {
        detectFaceCount = 0
        self.imageState.textColor = UIColor.label
        self.imageState.text = message
    }
}
